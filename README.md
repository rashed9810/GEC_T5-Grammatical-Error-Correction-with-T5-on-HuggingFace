# ðŸš€ GEC-T5: Grammatical Error Correction with T5

[![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Hugging%20Face-yellow.svg)](https://huggingface.co/)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0+-orange.svg)](https://www.tensorflow.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

> **State-of-the-art grammatical error correction powered by T5 transformers**

This project leverages the power of T5 models from Hugging Face to deliver cutting-edge grammatical error correction for English text.

## âœ¨ Features

- **High-Accuracy Corrections**: Fine-tuned T5 model achieving impressive results on standard GEC benchmarks
- **Production-Ready**: Optimized for both performance and accuracy
- **GPU-Accelerated**: Takes full advantage of GPU environments like Google Colab
- **Easy Integration**: Simple API for incorporating into existing NLP pipelines
- **Comprehensive Metrics**: Evaluation using industry-standard BLEU scores

## ðŸ“‹ Prerequisites

Before diving in, ensure you have:

- Python 3.7+
- TensorFlow (GPU support recommended)
- ðŸ¤— Transformers
- Datasets library
- SentencePiece
- Evaluate library
- SacreBLEU

### Quick Install

```bash
pip install transformers datasets evaluate sentencepiece sacrebleu
```

## ðŸ”§ Setup

### Clone the Repository

```bash
git clone https://github.com/rashed9810/GEC_T5-Grammatical-Error-Correction-with-T5-on-HuggingFace.git
cd GEC_T5-Grammatical-Error-Correction-with-T5-on-HuggingFace
```

### Environment

For optimal performance, run in a GPU-enabled environment. This project is specifically optimized for environments like Google Colab with T4 GPUs.

### Dataset

We use the `leslyarun/c4_200m_gec_train100k_test25k` dataset from Hugging Face, which is automatically downloaded during execution.

## ðŸš€ Usage

### Training the Model

Our implementation fine-tunes the `t5-small` model with these optimized parameters:

| Parameter | Value |
|-----------|-------|
| Batch Size | 64 |
| Max Sequence Length | 128 tokens |
| Training Epochs | 5 |
| Optimizer | Adam |
| Learning Rate | 2e-5 |

#### Training Code

```python
from transformers import T5TokenizerFast, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, create_optimizer
from datasets import load_dataset
import tensorflow as tf
import evaluate

# Load tokenizer and model
model_id = "t5-small"
tokenizer = T5TokenizerFast.from_pretrained(model_id)
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)

# Load dataset
dataset = load_dataset("leslyarun/c4_200m_gec_train100k_test25k")
BATCH_SIZE = 64
MAX_LENGTH = 128

# Preprocess function
def preprocess_function(examples):
    inputs = [example for example in examples['input']]
    targets = [example for example in examples['output']]
    model_inputs = tokenizer(inputs, text_target=targets, max_length=MAX_LENGTH, truncation=True)
    return model_inputs

# Tokenize dataset
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)

# Prepare TF datasets
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="tf")
train_dataset = tokenized_dataset["train"].to_tf_dataset(shuffle=True, batch_size=BATCH_SIZE, collate_fn=data_collator)
val_dataset = tokenized_dataset["test"].to_tf_dataset(shuffle=False, batch_size=BATCH_SIZE, collate_fn=data_collator)

# Compile and train
num_epochs = 5
num_train_steps = len(train_dataset) * num_epochs
optimizer, _ = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=num_train_steps)
model.compile(optimizer=optimizer)
model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)

# Save weights
model.save_weights('t5-small-gec.h5')
```

### Inference

Quickly correct grammatical errors in any text:

```python
# Example usage
wrong_english = ["Dady hav'e eateing her foot", "i used to like to swimming"]
tokenized = tokenizer(wrong_english, padding="longest", max_length=MAX_LENGTH, truncation=True, return_tensors='tf')
outputs = model.generate(**tokenized, max_length=128)
for i, text in enumerate(wrong_english):
    print(f"{text} ---> {tokenizer.decode(outputs[i], skip_special_tokens=True)}")
```

## ðŸ“Š Results

After fine-tuning for 5 epochs, our model achieves:

| Metric | Score |
|--------|-------|
| Training Loss | ~0.7878 |
| Validation Loss | ~0.7198 |

Sample corrections:

| Input | Corrected Output |
|-------|------------------|
| Dady hav'e eateing her foot | Daddy has eaten her food |
| i used to like to swimming | I used to like swimming |

